{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f92fe637-25fb-4d1c-aab1-9b608bad6f4d",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5285a0e1-36a8-4b53-ab79-0add729715db",
   "metadata": {},
   "source": [
    "### Spark Session Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ead2d3c8-5710-48b1-ba06-40e7111014e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b1c656e-51c7-417c-9121-f899aae6ea76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/16 19:31:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/04/16 19:31:35 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('modelTraining').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1475d7e-21bf-4cac-a576-49a9b3aa65dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loading will change once data is hosted on hadoop? loading from local for now\n",
    "from pyspark import SparkFiles\n",
    "df = spark.read.parquet('prepped_data.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea52d8e-7606-4137-9762-3677f46c6c2e",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "489acd23-1ca4-42cd-b2b3-fa02dd7197c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = df.randomSplit([0.7, 0.3], seed = 101)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e11186-80f1-46bd-b71c-bdc57a9e397d",
   "metadata": {},
   "source": [
    "### Model Importing and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc74e917-c271-4aef-a8bb-db11b787a9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import models\n",
    "from pyspark.ml.classification import NaiveBayes, RandomForestClassifier, LogisticRegression, GBTClassifier\n",
    "\n",
    "#set up models with default settings\n",
    "nb = NaiveBayes()\n",
    "rf = RandomForestClassifier(seed = 101)\n",
    "lr = LogisticRegression()\n",
    "gb = GBTClassifier(seed = 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdabdbf2-909a-46b0-bc77-45c4a29945ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#eval set up\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "acc_evaluator = BinaryClassificationEvaluator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c37c854-9a7d-4ed8-a02f-87c318f0adf7",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29c5a144-450a-4898-8597-e86ddbbdce1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "#naive bayes\n",
    "grid = ParamGridBuilder().addGrid(nb.smoothing, [0.0, 0.25, 0.5, 0.75, 1.0, 1.25, 1.5]).build()\n",
    "nb_cv = CrossValidator(estimator = nb, estimatorParamMaps = grid, evaluator = acc_evaluator, numFolds = 5, parallelism = 4, seed = 101)\n",
    "\n",
    "#random forest\n",
    "grid = ParamGridBuilder().addGrid(rf.numTrees, range(10, 60, 10)).addGrid(rf.maxDepth, range(1, 11)).build()\n",
    "rf_cv = CrossValidator(estimator = rf, estimatorParamMaps = grid, evaluator = acc_evaluator, numFolds = 5, parallelism = 4, seed = 101)\n",
    "\n",
    "#logistic regression\n",
    "grid = ParamGridBuilder().addGrid(lr.regParam, [0.01, 0.1, 0.5, 1.0]).addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]).build()\n",
    "lr_cv = CrossValidator(estimator = lr, estimatorParamMaps = grid, evaluator = acc_evaluator, numFolds = 5, parallelism = 4, seed = 101)\n",
    "\n",
    "#gradiant boosted trees\n",
    "grid = ParamGridBuilder().addGrid(gb.maxDepth, [3, 5, 7]).addGrid(gb.maxIter, [10, 20, 30]).addGrid(gb.stepSize, [0.05, 0.1, 0.2]).build()\n",
    "gb_cv = CrossValidator(estimator = gb, estimatorParamMaps = grid, evaluator = acc_evaluator, numFolds = 5, parallelism = 4, seed = 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e7ffbb5-51e3-4ab7-a6ef-1419712f965d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/16 19:56:42 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/04/16 19:56:42 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "#run predictions\n",
    "nb_cv_model = nb_cv.fit(train_data)\n",
    "nb_cv_pred = nb_cv_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "821a6aac-1d6c-49f0-9291-e5e6314f4aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/16 20:08:57 WARN DAGScheduler: Broadcasting large task binary with size 1107.0 KiB\n",
      "25/04/16 20:09:07 WARN DAGScheduler: Broadcasting large task binary with size 1030.9 KiB\n",
      "25/04/16 20:09:09 WARN DAGScheduler: Broadcasting large task binary with size 1030.9 KiB\n",
      "25/04/16 20:09:10 WARN DAGScheduler: Broadcasting large task binary with size 1393.0 KiB\n",
      "25/04/16 20:09:22 WARN DAGScheduler: Broadcasting large task binary with size 1292.7 KiB\n",
      "25/04/16 20:09:24 WARN DAGScheduler: Broadcasting large task binary with size 1292.7 KiB\n",
      "25/04/16 20:09:24 WARN DAGScheduler: Broadcasting large task binary with size 1737.9 KiB\n",
      "25/04/16 20:09:24 WARN DAGScheduler: Broadcasting large task binary with size 1160.4 KiB\n",
      "25/04/16 20:09:55 WARN DAGScheduler: Broadcasting large task binary with size 1141.7 KiB\n",
      "25/04/16 20:10:04 WARN DAGScheduler: Broadcasting large task binary with size 1058.0 KiB\n",
      "25/04/16 20:10:05 WARN DAGScheduler: Broadcasting large task binary with size 1058.0 KiB\n",
      "25/04/16 20:10:06 WARN DAGScheduler: Broadcasting large task binary with size 1439.8 KiB\n",
      "25/04/16 20:10:18 WARN DAGScheduler: Broadcasting large task binary with size 1357.6 KiB\n",
      "25/04/16 20:10:19 WARN DAGScheduler: Broadcasting large task binary with size 1357.6 KiB\n",
      "25/04/16 20:10:19 WARN DAGScheduler: Broadcasting large task binary with size 1850.5 KiB\n",
      "25/04/16 20:10:20 WARN DAGScheduler: Broadcasting large task binary with size 1204.7 KiB\n",
      "25/04/16 20:10:49 WARN DAGScheduler: Broadcasting large task binary with size 1107.1 KiB\n",
      "25/04/16 20:10:57 WARN DAGScheduler: Broadcasting large task binary with size 1055.5 KiB\n",
      "25/04/16 20:10:59 WARN DAGScheduler: Broadcasting large task binary with size 1055.5 KiB\n",
      "25/04/16 20:11:00 WARN DAGScheduler: Broadcasting large task binary with size 1433.6 KiB\n",
      "25/04/16 20:11:11 WARN DAGScheduler: Broadcasting large task binary with size 1285.6 KiB\n",
      "25/04/16 20:11:11 WARN DAGScheduler: Broadcasting large task binary with size 1285.6 KiB\n",
      "25/04/16 20:11:11 WARN DAGScheduler: Broadcasting large task binary with size 1750.0 KiB\n",
      "25/04/16 20:11:12 WARN DAGScheduler: Broadcasting large task binary with size 1163.9 KiB\n",
      "25/04/16 20:11:38 WARN DAGScheduler: Broadcasting large task binary with size 1081.8 KiB\n",
      "25/04/16 20:11:45 WARN DAGScheduler: Broadcasting large task binary with size 1031.0 KiB\n",
      "25/04/16 20:11:47 WARN DAGScheduler: Broadcasting large task binary with size 1031.0 KiB\n",
      "25/04/16 20:11:47 WARN DAGScheduler: Broadcasting large task binary with size 1400.6 KiB\n",
      "25/04/16 20:11:56 WARN DAGScheduler: Broadcasting large task binary with size 1280.1 KiB\n",
      "25/04/16 20:11:58 WARN DAGScheduler: Broadcasting large task binary with size 1280.1 KiB\n",
      "25/04/16 20:11:58 WARN DAGScheduler: Broadcasting large task binary with size 1740.8 KiB\n",
      "25/04/16 20:11:59 WARN DAGScheduler: Broadcasting large task binary with size 1142.1 KiB\n",
      "25/04/16 20:12:26 WARN DAGScheduler: Broadcasting large task binary with size 1028.9 KiB\n",
      "25/04/16 20:12:34 WARN DAGScheduler: Broadcasting large task binary with size 1025.9 KiB\n",
      "25/04/16 20:12:35 WARN DAGScheduler: Broadcasting large task binary with size 1025.9 KiB\n",
      "25/04/16 20:12:36 WARN DAGScheduler: Broadcasting large task binary with size 1376.3 KiB\n",
      "25/04/16 20:12:44 WARN DAGScheduler: Broadcasting large task binary with size 1243.1 KiB\n",
      "25/04/16 20:12:45 WARN DAGScheduler: Broadcasting large task binary with size 1243.1 KiB\n",
      "25/04/16 20:12:45 WARN DAGScheduler: Broadcasting large task binary with size 1660.6 KiB\n",
      "25/04/16 20:12:46 WARN DAGScheduler: Broadcasting large task binary with size 1112.7 KiB\n",
      "25/04/16 20:12:51 WARN DAGScheduler: Broadcasting large task binary with size 1313.1 KiB\n",
      "25/04/16 20:12:51 WARN DAGScheduler: Broadcasting large task binary with size 1825.6 KiB\n"
     ]
    }
   ],
   "source": [
    "rf_cv_model = rf_cv.fit(train_data)\n",
    "rf_cv_pred = rf_cv_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7b3a3a6-afe8-4105-a2eb-29be07b737e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_cv_model = lr_cv.fit(train_data)\n",
    "lr_cv_pred = lr_cv_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8896ac84-d990-4e19-a71d-5c2c7c098ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/16 20:39:51 WARN BlockManager: Asked to remove block broadcast_36734_piece0, which does not exist\n",
      "25/04/16 20:41:58 WARN BlockManager: Asked to remove block broadcast_45033, which does not exist\n"
     ]
    }
   ],
   "source": [
    "gb_cv_model = gb_cv.fit(train_data)\n",
    "gb_cv_pred = gb_cv_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffd3db7f-f687-49c5-a4f2-9933e4b1520e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaiveBayesModel: uid=NaiveBayes_be54f111b6de, modelType=multinomial, numClasses=2, numFeatures=4\n",
      "Naive Bayes Accuracy: 0.440370162795473\n"
     ]
    }
   ],
   "source": [
    "#results\n",
    "print(nb_cv_model.bestModel)\n",
    "\n",
    "nb_accuracy = acc_evaluator.evaluate(nb_cv_pred)\n",
    "print(f\"Naive Bayes Accuracy: {nb_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62426fa7-3e6a-4f78-84a2-24de01c7428c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassificationModel: uid=RandomForestClassifier_eafe4c4451a0, numTrees=50, numClasses=2, numFeatures=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/16 20:15:14 WARN DAGScheduler: Broadcasting large task binary with size 1189.2 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.9921085607065917\n"
     ]
    }
   ],
   "source": [
    "print(rf_cv_model.bestModel)\n",
    "\n",
    "rf_accuracy = acc_evaluator.evaluate(rf_cv_pred)\n",
    "print(f\"Random Forest Accuracy: {rf_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23d03b82-d012-423e-87bb-2be0a1d32298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegressionModel: uid=LogisticRegression_59d371609159, numClasses=2, numFeatures=4\n",
      "Logistic Regression Accuracy: 0.8421164330874442\n"
     ]
    }
   ],
   "source": [
    "print(lr_cv_model.bestModel)\n",
    "\n",
    "lr_accuracy = acc_evaluator.evaluate(lr_cv_pred)\n",
    "print(f\"Logistic Regression Accuracy: {lr_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a76a7bfc-7258-48a7-9b94-662596ae80ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBTClassificationModel: uid = GBTClassifier_3bea227d58d4, numTrees=30, numClasses=2, numFeatures=4\n",
      "Gradient Boosted Accuracy: 0.9922825767164476\n"
     ]
    }
   ],
   "source": [
    "print(gb_cv_model.bestModel)\n",
    "\n",
    "gb_accuracy = acc_evaluator.evaluate(gb_cv_pred)\n",
    "print(f\"Gradient Boosted Accuracy: {gb_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0952de60-cbf0-4587-acf9-be2272fc86dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save predictions for gradio\n",
    "\n",
    "#udf to expand features vector and get correct confidence\n",
    "from pyspark.ml.linalg import VectorUDT\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "\n",
    "vector_to_array_udf = udf(lambda vec: vec.toArray().tolist(), ArrayType(DoubleType()))\n",
    "\n",
    "def get_confidence(probability_vector, prediction):\n",
    "    return float(probability_vector[int(prediction)])\n",
    "confidence_udf = udf(get_confidence, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15aed7ad-bb58-4768-96ab-1e536de2134d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "#save nb\n",
    "nb_pred_expanded = nb_cv_pred.withColumn('features', vector_to_array_udf(col('features')))\n",
    "feature_size = len(nb_pred_expanded.select('features').first()['features'])\n",
    "for i in range(feature_size):\n",
    "    nb_pred_expanded = nb_pred_expanded.withColumn(f\"feature_{i}\", col('features')[i])\n",
    "\n",
    "nb_pred_final = nb_pred_expanded.withColumn('confidence', confidence_udf(col('probability'), col('prediction'))).drop('features', 'probability', 'rawPrediction')\n",
    "\n",
    "nb_pred_final.coalesce(1).write.csv('nb_predictions.csv', header = True, mode = 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c879b79-9055-45e1-8ca9-4831d5053e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/16 20:15:19 WARN DAGScheduler: Broadcasting large task binary with size 1415.3 KiB\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "#save rf\n",
    "rf_pred_expanded = rf_cv_pred.withColumn('features', vector_to_array_udf(col('features')))\n",
    "feature_size = len(rf_pred_expanded.select('features').first()['features'])\n",
    "for i in range(feature_size):\n",
    "    rf_pred_expanded = rf_pred_expanded.withColumn(f\"feature_{i}\", col('features')[i])\n",
    "\n",
    "rf_pred_final = rf_pred_expanded.withColumn('confidence', confidence_udf(col('probability'), col('prediction'))).drop('features', 'probability', 'rawPrediction')\n",
    "\n",
    "rf_pred_final.coalesce(1).write.csv('rf_predictions.csv', header = True, mode = 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93586b62-e598-47e4-9a4e-588e50a4064f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "#save lr\n",
    "lr_pred_expanded = lr_cv_pred.withColumn('features', vector_to_array_udf(col('features')))\n",
    "feature_size = len(lr_pred_expanded.select('features').first()['features'])\n",
    "for i in range(feature_size):\n",
    "    lr_pred_expanded = lr_pred_expanded.withColumn(f\"feature_{i}\", col('features')[i])\n",
    "\n",
    "lr_pred_final = lr_pred_expanded.withColumn('confidence', confidence_udf(col('probability'), col('prediction'))).drop('features', 'probability', 'rawPrediction')\n",
    "\n",
    "lr_pred_final.coalesce(1).write.csv('lr_predictions.csv', header = True, mode = 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2995b383-8be0-4ffa-aae7-bac0d9d19f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "#save gb\n",
    "gb_pred_expanded = gb_cv_pred.withColumn('features', vector_to_array_udf(col('features')))\n",
    "feature_size = len(gb_pred_expanded.select('features').first()['features'])\n",
    "for i in range(feature_size):\n",
    "    gb_pred_expanded = gb_pred_expanded.withColumn(f\"feature_{i}\", col('features')[i])\n",
    "\n",
    "gb_pred_final = gb_pred_expanded.withColumn('confidence', confidence_udf(col('probability'), col('prediction'))).drop('features', 'probability', 'rawPrediction')\n",
    "\n",
    "gb_pred_final.coalesce(1).write.csv('gb_predictions.csv', header=True, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de84142f-5112-42d6-b2aa-3909665d6ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all resulting csv files manually renamed and moved to root project folder for easier use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53340426-af2e-45d9-b3bb-a336044203b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "gb_pred_df = pd.read_csv('gb_predictions.csv')\n",
    "lr_pred_df = pd.read_csv('lr_predictions.csv')\n",
    "nb_pred_df = pd.read_csv('nb_predictions.csv')\n",
    "rf_pred_df = pd.read_csv('rf_predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e776e29d-b4f6-49a7-8250-e5040cdb567d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>prediction</th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.08</td>\n",
       "      <td>95035.0</td>\n",
       "      <td>1.354386e+09</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.978845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.08</td>\n",
       "      <td>165556.0</td>\n",
       "      <td>1.347217e+09</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.980035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.15</td>\n",
       "      <td>653.0</td>\n",
       "      <td>1.363595e+09</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.973805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.17</td>\n",
       "      <td>2836.0</td>\n",
       "      <td>1.354976e+09</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.975929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.18</td>\n",
       "      <td>190249.0</td>\n",
       "      <td>1.336133e+09</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.975929</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  prediction  feature_0  feature_1     feature_2  feature_3  \\\n",
       "0      0         0.0       1.08    95035.0  1.354386e+09        5.0   \n",
       "1      0         0.0       1.08   165556.0  1.347217e+09        7.0   \n",
       "2      0         0.0       1.15      653.0  1.363595e+09        5.0   \n",
       "3      0         0.0       1.17     2836.0  1.354976e+09        3.0   \n",
       "4      0         0.0       1.18   190249.0  1.336133e+09        3.0   \n",
       "\n",
       "   confidence  \n",
       "0    0.978845  \n",
       "1    0.980035  \n",
       "2    0.973805  \n",
       "3    0.975929  \n",
       "4    0.975929  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2df49fa-0432-4b70-b3d7-c94ca29126ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cols = ['true', 'prediction', 'amt', 'city_pop', 'unix_time', 'category_index', 'confidence']\n",
    "gb_pred_df.columns = new_cols\n",
    "lr_pred_df.columns = new_cols\n",
    "nb_pred_df.columns = new_cols\n",
    "rf_pred_df.columns = new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24016a5c-6c80-4774-90ed-49acdae5001c",
   "metadata": {},
   "outputs": [],
   "source": [
    "join_cols = ['true', 'amt', 'city_pop', 'unix_time', 'category_index']\n",
    "\n",
    "gb_trimmed = gb_pred_df[join_cols + ['prediction', 'confidence']].rename(columns = {\n",
    "    'prediction': 'prediction_gb',\n",
    "    'confidence': 'confidence_gb'\n",
    "})\n",
    "\n",
    "lr_trimmed = lr_pred_df[join_cols + ['prediction', 'confidence']].rename(columns = {\n",
    "    'prediction': 'prediction_lr',\n",
    "    'confidence': 'confidence_lr'\n",
    "})\n",
    "\n",
    "nb_trimmed = nb_pred_df[join_cols + ['prediction', 'confidence']].rename(columns = {\n",
    "    'prediction': 'prediction_nb',\n",
    "    'confidence': 'confidence_nb'\n",
    "})\n",
    "\n",
    "rf_trimmed = rf_pred_df[join_cols + ['prediction', 'confidence']].rename(columns = {\n",
    "    'prediction': 'prediction_rf',\n",
    "    'confidence': 'confidence_rf'\n",
    "})\n",
    "\n",
    "pred_df = gb_trimmed.merge(lr_trimmed, on = join_cols)\n",
    "pred_df = pred_df.merge(nb_trimmed, on = join_cols)\n",
    "pred_df = pred_df.merge(rf_trimmed, on = join_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff378726-9288-445c-915e-8ab38f979c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.to_pickle('combined_predictions.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "776a5185-1ccc-4cc3-bf12-dabfcdeca13c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true</th>\n",
       "      <th>amt</th>\n",
       "      <th>city_pop</th>\n",
       "      <th>unix_time</th>\n",
       "      <th>category_index</th>\n",
       "      <th>prediction_gb</th>\n",
       "      <th>confidence_gb</th>\n",
       "      <th>prediction_lr</th>\n",
       "      <th>confidence_lr</th>\n",
       "      <th>prediction_nb</th>\n",
       "      <th>confidence_nb</th>\n",
       "      <th>prediction_rf</th>\n",
       "      <th>confidence_rf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.08</td>\n",
       "      <td>95035.0</td>\n",
       "      <td>1.354386e+09</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.978845</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.684297</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.993297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1.08</td>\n",
       "      <td>165556.0</td>\n",
       "      <td>1.347217e+09</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.980035</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.699808</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.970565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1.15</td>\n",
       "      <td>653.0</td>\n",
       "      <td>1.363595e+09</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.973805</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.690551</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.980560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1.17</td>\n",
       "      <td>2836.0</td>\n",
       "      <td>1.354976e+09</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.975929</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.664210</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.986810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1.18</td>\n",
       "      <td>190249.0</td>\n",
       "      <td>1.336133e+09</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.975929</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.650739</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.984470</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   true   amt  city_pop     unix_time  category_index  prediction_gb  \\\n",
       "0     0  1.08   95035.0  1.354386e+09             5.0            0.0   \n",
       "1     0  1.08  165556.0  1.347217e+09             7.0            0.0   \n",
       "2     0  1.15     653.0  1.363595e+09             5.0            0.0   \n",
       "3     0  1.17    2836.0  1.354976e+09             3.0            0.0   \n",
       "4     0  1.18  190249.0  1.336133e+09             3.0            0.0   \n",
       "\n",
       "   confidence_gb  prediction_lr  confidence_lr  prediction_nb  confidence_nb  \\\n",
       "0       0.978845            0.0       0.684297            0.0            1.0   \n",
       "1       0.980035            0.0       0.699808            1.0            1.0   \n",
       "2       0.973805            0.0       0.690551            0.0            1.0   \n",
       "3       0.975929            0.0       0.664210            0.0            1.0   \n",
       "4       0.975929            0.0       0.650739            1.0            1.0   \n",
       "\n",
       "   prediction_rf  confidence_rf  \n",
       "0            0.0       0.993297  \n",
       "1            0.0       0.970565  \n",
       "2            0.0       0.980560  \n",
       "3            0.0       0.986810  \n",
       "4            0.0       0.984470  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf96fa8-bd45-41fc-81b2-bf273e64a52d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
